{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Spark and Machine Learning\n",
    "---\n",
    "\n",
    "In this tutorial we will go from scratch through the whole process for using machine learning with Spark. We will use Python (PySpark) as language for our notebook. We will adopt an iterative approach for the process, trying to improve our analysis and knowledge on each iteration. \n",
    "\n",
    "Some of the activities we will see are:\n",
    "* Get the data\n",
    "* Prepare the data for Spark ML\n",
    "* Visualize the data\n",
    "* Data cleaning\n",
    "* Feature engineering\n",
    "* Train models using several ML algorithms\n",
    "* Evaluate models\n",
    "* Send the results to Kaggle (this is optional)\n",
    "\n",
    "For this example, we are going to use the datasets from the ATLAS Higgs Boson Machine Learning Challenge. This challenge was published by the ATLAS experiment in May 2014 in the Kaggle platform. The challenge closed in September 2014 but we can still use the data for learning purposes. We can also submit our predictions to Kaggle to see how good they are.\n",
    "\n",
    "\n",
    "## ATLAS Higgs Boson Machine Learning Challenge\n",
    "\n",
    "\n",
    "The goal of the Higgs Boson Machine Learning Challenge was to explore the potential of advanced machine learning methods to improve the discovery significance of the experiment. Using simulated data with features characterizing events detected by ATLAS, your task is to classify events where there is a Higgs boson decay into two tau particles (known as \"signal\") versus events where this decay is not present (known as \"background\"). \n",
    "\n",
    "![Higgs Challenge](https://kaggle2.blob.core.windows.net/competitions/kaggle/3887/media/ATLASEXP_image.png)\n",
    "link: https://www.kaggle.com/c/higgs-boson\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "For this challenge we are provided with the following files:\n",
    "\n",
    "* training.csv - Training set of 250000 events, with an ID column, 30 feature columns, a weight column and a label column.\n",
    "* test.csv - Test set of 550000 events with an ID column and 30 feature columns\n",
    "* random_submission - Sample submission file in the correct format. File format is described on https://www.kaggle.com/c/higgs-boson/details/evaluation\n",
    "* HiggsBosonCompetition_AMSMetric - Python script to calculate the competition evaluation metric.\n",
    "\n",
    "Some additional details about the dataset:\n",
    "\n",
    "* All variables are floating point, except PRI_jet_num which is integer\n",
    "* Variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector\n",
    "* Variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS\n",
    "* In some observations, some features are meaningless or cannot be computed. In these cases their value is −999.0, which is outside the normal range of all variables\n",
    "\n",
    "## Environment setup and resources\n",
    "\n",
    "There are several resources that are useful or required for this tutorial. \n",
    "\n",
    "The SWAN service provides on-demand notebooks already prepared to use with Spark. For this tutorial, you can get access to the SWAN notebooks subscribing to the CERN e-group \"hadoop-tutorials-2016-s1\". You also need a CERNBox account to use the SWAN notebooks. \n",
    "*NOTE*: It is assumed that you will be using SWAN notebooks in this tutorial.\n",
    "\n",
    "If you want to send the results for the evaluation in Kaggle you need first to sign up. This is optional for the tutorial.\n",
    "\n",
    "In the following list you can find the links for the mentioned services:\n",
    "* SWAN: https://swan.cern.ch/\n",
    "* CERNBox: https://cernbox.cern.ch/\n",
    "* CERN e-groups: https://e-groups.cern.ch/\n",
    "* Github: https://github.com/cerndb/hadoop-tutorials\n",
    "* Gitlab: https://gitlab.cern.ch/db/hadoop-tutorials-2016\n",
    "* Kaggle: https://www.kaggle.com/\n",
    "\n",
    "The following links contain useful documentation and reference for this tutorial:\n",
    "* Spark MLlib documentation: http://spark.apache.org/docs/1.6.1/mllib-guide.html\n",
    "* PySpark reference: http://spark.apache.org/docs/1.6.1/api/python/index.html\n",
    "\n",
    "<style>\n",
    "p.info\n",
    "{\n",
    "     color: #616E14;\n",
    "     border: solid 1px #DDC82D;\n",
    "     background-color: #FCF8D1;\n",
    "     -moz-border-radius: 6px;\n",
    "     -webkit-border-radius: 6px;\n",
    "     border-radius: 6px;\n",
    "     padding: 14px 20px;\n",
    "     mc-auto-number-format: '{b}Note: {/b}';\n",
    "}\n",
    "</style>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using PySpark for Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "## Get the data\n",
    "\n",
    "First, we need to get the data for our analysis. The data can be downloaded from the Kaggle platform but for simplicity it is also available as a zip file shared in CERNBox.\n",
    "\n",
    "\n",
    "<p class=info><b>Info:</b> If the file is external we can use python for example to download it:</p>\n",
    "\n",
    "```python\n",
    "import urllib\n",
    "opener = urllib.URLopener(urlToMyFile)\n",
    "opener.retrieve(url, \"/tmp/\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "Let's create our working directory, copy the zip file and extract in your working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# declare few convinient variables\n",
    "home = os.environ['HOME']+\"/\"\n",
    "wd = home+\"spark-tutorial-ml/\"\n",
    "zipFilename = \"kaggle-higgs.zip\"\n",
    "remoteFilename = \"/eos/user/a/aromerom/Public/kaggle-higgs/kaggle-higgs.zip\"\n",
    "seed = 12345L\n",
    "\n",
    "# configure pandas options\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 35\n",
    "pd.options.display.max_colwidth = 35\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the working directory and change dir\n",
    "%cd $home\n",
    "%rm -rf $wd # uncomment this line if you want to remove everything in your wd\n",
    "%mkdir -p $wd\n",
    "%cd $wd\n",
    "\n",
    "# copy the data into your working directory\n",
    "%cp $remoteFilename $wd\n",
    "%ll $wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Extract the file\n",
    "zip_ref = zipfile.ZipFile(file=wd+zipFilename, mode='r')\n",
    "zip_ref.extractall(wd)\n",
    "zip_ref.close()\n",
    "%ll $wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data we can start working with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets and prepare them for Spark ML\n",
    "\n",
    "The first thing we have to do to use Spark is to create the SparkContext. We also need to create the SQLContext because we are going to work with DataFrames.\n",
    "Once the context is created, we can read the train and test datasets.\n",
    "\n",
    "<p class=\"info\"><b>Note:</b> We have added the package \"spark-csv\" before creating the SparkContext. This package provides utils to create DataFrames from csv files and save them back in csv format. If spark-csv package is not available you can always read the files using the traditional textFile function and create a DataFrame from the RDD:\n",
    "</p>\n",
    "```python\n",
    "\n",
    "trainRDD = sc.textFile(datapath+\"training.csv\").map(lambda line: line.split(\",\"))\n",
    "train = sqlContext.createDataFrame(trainRDD)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# add spark-csv packages\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'\n",
    "\n",
    "# create spark and sql context\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Spark contexts have been properly create we should be able now to read our datasets. We will use the utils available in the `com.databricks.spark.csv` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read train and test datasets\n",
    "train = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true').load(wd+\"training.csv\")\n",
    "    \n",
    "test = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true').load(wd+\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to check that the data was correctly parsed into DataFrame. We can check the data types using `printSchema()`. We can print the content of a Spark DataFrame using `show()`.\n",
    "\n",
    "<p class=\"info\"><b>Note:</b> The output of some Spark fuctions like `show()` might return plain text that can be difficult to read depending on the number of columns of your Dataframe. In those cases, you can always convert to Pandas using `toPandas()` which has better output features for Jupyter notebooks.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.limit(1).show() # This will show the content in plain text\n",
    "train.limit(5).toPandas() # this shows the content in a nice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to shape our dataset before we can use some Spark ML algorithms with it. First, we need to create a new column containing a Vector with all the features. That vector will be the input features used by the ML for the model training. \n",
    "\n",
    "We can do it using the VectorAssembler class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "\n",
    "# get the list of features\n",
    "featureList = train.columns\n",
    "featureList.remove('Label')\n",
    "featureList.remove('EventId')\n",
    "featureList.remove('Weight')\n",
    "\n",
    "# the assembler will create a single column with vector of features\n",
    "assembler = VectorAssembler(inputCols=featureList, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler.transform(train.limit(5)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert our labels (that is \"s\" for signal\" and \"b\" for background\") into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this converts the label into an index, ncessary for the machine learning\n",
    "labelIndexer = StringIndexer(inputCol=\"Label\", outputCol=\"indexedLabel\").fit(train)\n",
    "\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", \n",
    "                               outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are now ready to apply some ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision Trees are a commonly used supervised machine learning algorithm used for classification and regression. The goal using decistion trees is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "In the following image we can see an example of a decisition tree model showing the survival of passengers on the Titanic dataset (\"sibsp\" is the number of spouses or siblings aboard). The figures under the leaves show the probability of survival and the percentage of observations in the leaf.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
    "\n",
    "The `spark.ml` implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dtClassifier = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, it is common to run a sequence of actions to process the data and train your models. Spark MLlib represents such a workflow as a Pipeline, which consists of a sequence of actions to be run in a specific order.\n",
    "\n",
    "In our case, we are going to create a Pipeline to run assembler->labelIndexer->decisiontree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# use pipeline as a container for the actions\n",
    "pipeline = Pipeline(stages=[assembler, labelIndexer, dtClassifier, labelConverter])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "dtModel = pipeline.fit(train)\n",
    "\n",
    "dtPrediction = dtModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the predictions made by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtPrediction.limit(5).select('EventId','features','rawPrediction','probability','prediction', 'predictedLabel')\\\n",
    "            .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our predictions in the format expected by Kaggle. We will create a function so it can be reused later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save a csv file with the format needed for submitting to Kaggle\n",
    "def prepareForKaggle(dataset, path, overwrite=False):\n",
    "    if overwrite:\n",
    "        %rm -rf $path\n",
    "    resultsFormatted = dataset.withColumn('RankOrder', dataset.EventId-349999)\\\n",
    "                              .select('EventId','RankOrder','predictedLabel')\n",
    "    resultsFormatted = resultsFormatted.withColumnRenamed('predictedLabel','Class')\n",
    "    resultsFormatted.coalesce(1).write.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"header\", \"true\").option(\"codec\",\"org.apache.hadoop.io.compress.GzipCodec\").save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepareForKaggle(dtPrediction, wd+\"results-dt-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an account in Kaggle you can upload the results and wait for the evaluation.\n",
    "\n",
    "Question: What is the default \"maxDepth\" parameter in the DecisionTreeClassifier? Does the result improve putting a higher value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleaning\n",
    "\n",
    "Our first ML model was generated with the raw training dataset. Nevertheless, in many (if not all) cases it is necessary a dedicated process to clean the data and ensure the quality of it. This process varies a lot depending on the dataset.\n",
    "\n",
    "In our example, we know that null values are represented by the value -999.0. Using the `com.databricks.spark.csv` package, we can specify which value in the dataset correspond to the null/missing value using the parameter `nullValue`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true', nullValue=-999.0).load(wd+\"training.csv\")\n",
    "    \n",
    "test = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true', nullValue=-999.0).load(wd+\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill those null values using `na.fill`. Let's have a look at the different values in the dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.describe().toPandas()\n",
    "train.na.fill(-999.0).describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many rows contain null values in our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.na.drop().count()\n",
    "test.na.drop().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Pandas we can also check the null values easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainpd = train.drop('EventId').drop('Weigth').toPandas() # remove unnecessary columns\n",
    "testpd = test.toPandas()\n",
    "\n",
    "pd.isnull(trainpd).sum()\n",
    "pd.isnull(testpd).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know some features are raw (PRI) and others are calculated for expert (DER) so we can check how good they are in comparison for the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresDER = ['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', \n",
    "              'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt',\n",
    "              'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality']\n",
    "\n",
    "featuresPRI = ['PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met',\n",
    "              'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta',\n",
    "              'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi',\n",
    "              'PRI_jet_all_pt']\n",
    "\n",
    "assemblerDER = VectorAssembler(inputCols=featuresDER, outputCol=\"features\")\n",
    "assemblerPRI = VectorAssembler(inputCols=featuresPRI, outputCol=\"features\")\n",
    "\n",
    "pipelineDER = Pipeline(stages=[assemblerDER, labelIndexer, dtClassifier, labelConverter])\n",
    "pipelinePRI = Pipeline(stages=[assemblerPRI, labelIndexer, dtClassifier, labelConverter])\n",
    "\n",
    "dtModelDER = pipelineDER.fit(train.na.fill(0))\n",
    "dtModelPRI = pipelinePRI.fit(train.na.fill(0))\n",
    "\n",
    "dtPredictionDER = dtModelDER.transform(test.na.fill(0))\n",
    "dtPredictionPRI = dtModelPRI.transform(test.na.fill(0))\n",
    "\n",
    "prepareForKaggle(dtPredictionDER, wd+\"results-dt-DER\", True) \n",
    "prepareForKaggle(dtPredictionPRI, wd+\"results-dt-PRI\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtPrediction.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtPrediction.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Visualizing the data it is an important method to get a better understanding of the dataset properties. Some characteristics that can be spotted quickly in a plot might be very difficult to see looking at the raw values. Python has a good number of libraries for data visualization. When working with large datasets in Spark, it can be very useful to sample the data and draw some plots using Python libs.\n",
    "\n",
    "Since our dataset it is not very big, we can just take the whole dataset for the plots. We are going to use plotly library to see how we can plot few things about our dataset.\n",
    "\n",
    "NOTE: We need to install a couple of python libs. Open a SWAN terminal and execute the following commands:\n",
    "`pip install --user plotly`\n",
    "`pip install --user cufflinks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import cufflinks as cf\n",
    "\n",
    "# connect to the notebook\n",
    "cf.set_config_file(offline=True)\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# define the pandas datasets\n",
    "trainpd = train.drop('EventId').drop('Weigth').toPandas() # remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainpd.DER_mass_transverse_met_lep.iplot(kind='histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainpd.iplot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we would like to check is the correlation of the features. Correlated features are usually good candidates to be discarded to keep only the \"independent\" features (likely will be better for the model training).\n",
    "\n",
    "Remember that \"correlation does not imply causation\". Two events can consistently correlate with each other but not have any causal relationship. An example is the relationship between reading ability and shoe size across the whole population of a country. In that example, larger shoe sizes correlate with better reading ability, but this is caused by the fact that young children have small feet and have not yet (or only recently) been taught to read.\n",
    "\n",
    "The corration can calculated directly in Spark using `stat.corr('column1','column2')`. Spark provides other statistical functions for dataframes like the covariance `stat.cov`, cross tabulation `stat.crosstab` or frequent items `stat.freqItems`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainpd.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainpd.corr().iplot(kind='heatmap', colorscale='spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "* K-fold cross-validation is to repeat the construction of the model on different subsets of the available training data and then evaluate the model only on data not seen during construction.\n",
    "* The process can be summarised as follow:\n",
    "\n",
    "   \n",
    "   * 1 - *The training data is randomly divided into k groups, or folds, of approximately equal size* \n",
    " \n",
    "   * 2 - *While (interations < k)*\n",
    "      * 2.1 - *Set fold k as a validation test*\n",
    "      * 2.1 - *Train the model using the remaining k − 1 folds* \n",
    "      * 2.3 - *Calculate the mean squared error on the fold k, MSE(k)*\n",
    "      * 2.4 - *Increment iterations* \n",
    "      * 2.5 - *Back to (2)*\n",
    "   \n",
    "   * 3 - *The k-fold CV estimate is the average of [MSE(1),MSE(2),..,MSE(k)]*\n",
    "   <br />\n",
    "   <br />\n",
    "   <img src=\"http://genome.tugraz.at/proclassify/help/pages/images/xv_folds.gif\">\n",
    "   <br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "* Most of the models you will using in Machine Learning have several parameters and in most of the cases there is no analytics formula to calculate an appropiate value.\n",
    "* Many of these paremeters control the complexity of the model, poor choices results in critial problems such as bad performance, over-fitting, etc. \n",
    "<br />\n",
    "<br />\n",
    "<img src=\"ModelTuning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let see how we can apply both concepts using Spark.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# fill null values\n",
    "train = train.na.fill(0)\n",
    "test = test.na.fill(0)\n",
    "\n",
    "#Set Model\n",
    "dtClassifierCV = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    "\n",
    "#Setting the pipeline\n",
    "pipelineCV = Pipeline(stages=[assembler, labelIndexer, dtClassifierCV])\n",
    "\n",
    "\n",
    "#Prepare de evaluator and CV configuration\n",
    "evaluatorCV = BinaryClassificationEvaluator(labelCol=\"indexedLabel\")\n",
    "paramGrid = ParamGridBuilder().addGrid(dtClassifierCV.maxBins, [5,10]).build()\n",
    "crossValidator = CrossValidator(estimator=pipelineCV, estimatorParamMaps=paramGrid, evaluator=evaluatorCV, numFolds=3)\n",
    "\n",
    "cvModel = crossValidator.fit(train)\n",
    "\n",
    "#Printing information about the best model\n",
    "dtBestModelCV = cvModel.bestModel.stages[2]\n",
    "\n",
    "print dtBestModelCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting using the best model\n",
    "dtPredictionCV = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the data in Kaggle\n",
    "prepareForKaggle(dtPredictionCV, wd+\"results-dt-CV-03\")\n",
    "#Result 2.41491"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "* Ensemble methods are techniques that combines multiple models to generate the final output.\n",
    "* Usually ensembles model clasification or regresion obtain better accurary than single models\n",
    "    * They are compable to model the different pontential behavoius within the original dataset\n",
    "* Esemble have been used since early 90s but in during the last years when they have gained popularity due to the results obtained in different machine learning competitions \n",
    "    * Netflix Challenge\n",
    "    * Kaggle\n",
    "   \n",
    "Some of the most common ways to generate ensembles:\n",
    "\n",
    "### Voting and Averaging\n",
    "* The outputs of the different model are combined using one of the following techniques\n",
    "    * Voting (classification)\n",
    "    * Weighted Voting (classification)\n",
    "    * Averaging (Regression)\n",
    "    * Weighted Averaging (Regression)\n",
    "\n",
    "### Stacking Models\n",
    "* Another machine learning method is use to calculate the final output (models combination)\n",
    "* The output of the members of the ensemble are used as input for the model that will calculate the final output\n",
    "\n",
    "### Bagging (bootstrap aggregation)\n",
    "* The method is fairly simple in structure and consists of the steps summarized in the following picture, which illustrates the bagging approach on a small sample containing n = 3 observations. Each bootstrap data set contains n observations, sampled with replacement from the original data set. Each bootstrap data set is used to obtain an estimate of α.\n",
    "<img src=\"Bagging.png\">\n",
    "* Then each model in the ensemble generates a prediction (α) which are averaged to give the bagged model’s prediction.\n",
    "\n",
    "### Boosting\n",
    "* Boosting works in a similar way than bagging, but in this case the models are grown sequentially: each model is trained using information from previous trained versions. \n",
    "* Boosting does not involve bootstrap sampling; instead each model is fit on a modified version of the original data set, which emphasize those samples that were misclassified.\n",
    "* Boosting tends to get better accuracy that bagging based models but also increase the risk of over-fitting\n",
    "* Let explain that with a simple example:\n",
    "  *  Suppose we have just 5 samples [1,2,3,4,5]\n",
    "  *  Initially each sample has a probablity of 1/5 to be sampled\n",
    "  *  After the 1st round [1,2,3] are well classified while [4,5] are misclassified\n",
    "      * Then the algorithm will modify the samples probability\n",
    "          * Decreasing it for [1,2,3]\n",
    "          * Incresing it for [4,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#Set Model\n",
    "rfClassifierCV = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\n",
    "\n",
    "#Setting the pipeline\n",
    "rfPipelineCV = Pipeline(stages=[assembler, labelIndexer, rfClassifierCV])\n",
    "\n",
    "\n",
    "#Prepare de evaluator and CV configuration\n",
    "evaluatorCV = BinaryClassificationEvaluator(labelCol=\"indexedLabel\")\n",
    "paramGrid = ParamGridBuilder().addGrid(rfClassifierCV.numTrees, [5,10,15]).build()\n",
    "crossValidator = CrossValidator(estimator=rfPipelineCV, estimatorParamMaps=paramGrid, evaluator=evaluatorCV, numFolds=3)\n",
    "\n",
    "cvModel = crossValidator.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting using the best model\n",
    "rfPredictionCV = cvModel.transform(test)\n",
    "\n",
    "#Getting the data in Kaggle\n",
    "prepareForKaggle(rfPredictionCV, wd+\"results-rf-CV-01\")\n",
    "#2.65322"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
