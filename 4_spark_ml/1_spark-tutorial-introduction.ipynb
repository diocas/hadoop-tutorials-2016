{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics with Apache Spark\n",
    "\n",
    "## Why Apache Spark for Machine Learning?\n",
    "\n",
    "- Bigger than memory datasets\n",
    "\n",
    "One does not have to think about sampling a smaller, but statistical significant fraction of the data in order to train a model on a machine. Since Spark is able to deliver any-size dataset to a model.\n",
    "\n",
    "- Compatibility\n",
    "\n",
    "Multiple languages are supported, and integrate well with Apache Spark (e.g., Pandas).\n",
    "\n",
    "- **General purpose!**\n",
    "\n",
    "Not only scalable Machine Learning, but also advanced data preparation; normalizing features, handling invalid values, constructing new features, etc...\n",
    "\n",
    "## MLlib\n",
    "\n",
    "- RDD-based API (spark.mllib)\n",
    "  - No new features, only bugfixes.\n",
    "  - Is expected to be removed in Spark 3.0.\n",
    "\n",
    "\n",
    "- DataFrame-based API (spark.ml)\n",
    "  - Friendlier, and uniform API compared to spark.mllib\n",
    "  \n",
    "## Main concepts\n",
    "\n",
    "In the following code snippets I will introduce concepts such as, a **dataframe**, **transformer**, **estimator**, and **pipeline** in Spark's \"new\" ML API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"CERN Spark ML tutorial: main concepts\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe\n",
    "\n",
    "In this example we assume we have a dataset consisting of blog-comments on machine learning. Every instance (a comment) is assigned with a specific label, which can be either negative (0.0), or positive (1.0). In this example, it is our job to train a classifier which should be able to classify new comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imagine having a dataset with comments on machine learning. \n",
    "# Every instance (example or row), is tagged with a positive (1.0) or a negative (0.0) label.\n",
    "dataset = sqlContext.createDataFrame([\n",
    "    (0L, \"robots will take over and destroy the world like skynet\", 0.0),\n",
    "    (1L, \"AI helps humanity solve many problems\", 1.0),\n",
    "    (2L, \"unsupervised learning is pretty cool you can do a lot of awesome stuff with it\", 1.0),\n",
    "    (3L, \"i think unsupervised learning is naive\", 0.0),\n",
    "    (4L, \"machine learning is just a hype\", 0.0),\n",
    "    (5L, \"machine learning is awesome\", 1.0)], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|robots will take ...|  0.0|\n",
      "|  1|AI helps humanity...|  1.0|\n",
      "|  2|unsupervised lear...|  1.0|\n",
      "|  3|i think unsupervi...|  0.0|\n",
      "|  4|machine learning ...|  0.0|\n",
      "|  5|machine learning ...|  1.0|\n",
      "+---+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers and estimators\n",
    "\n",
    "**tldr;** a *transfomer* is responsible for changing the structure, or contents of a DataFrame. For example, in Spark, a trained machine learning model is a transformer. Since this model will add a prediction (default name) to every instance in the DataFrame. An *estimator* is basically an abstraction which computes some parameters based on the provided DataFrame. This could be for example, the learning algorithm, or a method which obtains some statistics, which in turn can be used by a model to transform a DataFrame.\n",
    "\n",
    "\n",
    "We first need to apply some preprocessing to the fulltext data before we can actually start training our model. In this (very) simple example, we show that we can easily apply some preprocessing with Spark. In the first step, we apply a tokenizer. This will parse the text and create a vector of words. Next, since frequent words like \"i\", \"a\", \"as\", ... are not really descriptive and can thus be filtered (reducing the dimensionality of the problem in progress). Now every comments is described by a vector of \"meaningfull\" words, we can start constructing our feature vectors for our machine learning model. In this example, we compute a term-frequency vector in order to represent an instance. One could for example use more complex features such as; TF-IDF vectors, Word Embeddings (see Word2Vec by Tomas Mikolov), ... Note that in order to compute a term frequency vector, one first needs the amount of words in the dictionary and a mapping of a string to the corresponding index. As a result, this cannot be done directly with a transformer (since you need to loop 2 times over the data).\n",
    "\n",
    "``cvModel = cv.fit(dataset)``\n",
    "\n",
    "This model wild hold all the information we need in order to construct the term frequency vectors. Finally, we apply this model to the dataset (thus, the model is a transformer), and obtain the processed dataset we will apply to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe:\n",
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|robots will take ...|  0.0|\n",
      "|  1|AI helps humanity...|  1.0|\n",
      "|  2|unsupervised lear...|  1.0|\n",
      "|  3|i think unsupervi...|  0.0|\n",
      "|  4|machine learning ...|  0.0|\n",
      "|  5|machine learning ...|  1.0|\n",
      "+---+--------------------+-----+\n",
      "\n",
      "After applying tokenizer:\n",
      "+---+--------------------+-----+--------------------+\n",
      "| id|                text|label|               words|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  0|robots will take ...|  0.0|[robots, will, ta...|\n",
      "|  1|AI helps humanity...|  1.0|[ai, helps, human...|\n",
      "|  2|unsupervised lear...|  1.0|[unsupervised, le...|\n",
      "|  3|i think unsupervi...|  0.0|[i, think, unsupe...|\n",
      "|  4|machine learning ...|  0.0|[machine, learnin...|\n",
      "|  5|machine learning ...|  1.0|[machine, learnin...|\n",
      "+---+--------------------+-----+--------------------+\n",
      "\n",
      "After applying stop-word remover:\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "| id|                text|label|               words|      filtered_words|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|  0|robots will take ...|  0.0|[robots, will, ta...|[robots, destroy,...|\n",
      "|  1|AI helps humanity...|  1.0|[ai, helps, human...|[ai, helps, human...|\n",
      "|  2|unsupervised lear...|  1.0|[unsupervised, le...|[unsupervised, le...|\n",
      "|  3|i think unsupervi...|  0.0|[i, think, unsupe...|[think, unsupervi...|\n",
      "|  4|machine learning ...|  0.0|[machine, learnin...|[machine, learnin...|\n",
      "|  5|machine learning ...|  1.0|[machine, learnin...|[machine, learnin...|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "\n",
      "After applying the count-vectorizer:\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "| id|                text|label|               words|      filtered_words|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|  0|robots will take ...|  0.0|[robots, will, ta...|[robots, destroy,...|(22,[7,13,18,19,2...|\n",
      "|  1|AI helps humanity...|  1.0|[ai, helps, human...|[ai, helps, human...|(22,[6,8,10,15,20...|\n",
      "|  2|unsupervised lear...|  1.0|[unsupervised, le...|[unsupervised, le...|(22,[0,1,2,4,9,11...|\n",
      "|  3|i think unsupervi...|  0.0|[i, think, unsupe...|[think, unsupervi...|(22,[0,1,12,14],[...|\n",
      "|  4|machine learning ...|  0.0|[machine, learnin...|[machine, learnin...|(22,[0,3,5,16],[1...|\n",
      "|  5|machine learning ...|  1.0|[machine, learnin...|[machine, learnin...|(22,[0,2,3],[1.0,...|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Most approaches cannot handle full-text. As a result, a word, or a piece of text needs to be described by a set\n",
    "# of features. In this example we will be using count vectors to describe a comment. However, one could also use\n",
    "# more advanced features such as word (or paragraph) embeddings (see Word2Vec by Tomas Mikolov for further details).\n",
    "\n",
    "# Clean the dataset if we rerun this part of the notebook.\n",
    "print(\"Original dataframe:\")\n",
    "dataset = dataset.select([dataset.id, dataset.text, dataset.label])\n",
    "dataset.show()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopWordRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"features\")\n",
    "\n",
    "# Show how the dataset evolves when it has been applied by every transformer.\n",
    "dataset = tokenizer.transform(dataset)\n",
    "print(\"After applying tokenizer:\")\n",
    "dataset.show()\n",
    "dataset = stopWordRemover.transform(dataset)\n",
    "print(\"After applying stop-word remover:\")\n",
    "dataset.show()\n",
    "cvModel = cv.fit(dataset)\n",
    "print(\"After applying the count-vectorizer:\")\n",
    "dataset = cvModel.transform(dataset)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "mlModel = nb.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "One could compact the above lines into a *pipeline*. This will encapsulate the above workflow so other data can be processed more easily without having to write the same code twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "| id|                text|label|               words|      filtered_words|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|  0|robots will take ...|  0.0|[robots, will, ta...|[robots, destroy,...|(22,[7,13,18,19,2...|\n",
      "|  1|AI helps humanity...|  1.0|[ai, helps, human...|[ai, helps, human...|(22,[6,8,10,15,20...|\n",
      "|  2|unsupervised lear...|  1.0|[unsupervised, le...|[unsupervised, le...|(22,[0,1,2,4,9,11...|\n",
      "|  3|i think unsupervi...|  0.0|[i, think, unsupe...|[think, unsupervi...|(22,[0,1,12,14],[...|\n",
      "|  4|machine learning ...|  0.0|[machine, learnin...|[machine, learnin...|(22,[0,3,5,16],[1...|\n",
      "|  5|machine learning ...|  1.0|[machine, learnin...|[machine, learnin...|(22,[0,2,3],[1.0,...|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "| id|                text|label|               words|      filtered_words|            features|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|  0|robots will take ...|  0.0|[robots, will, ta...|[robots, destroy,...|(22,[7,13,18,19,2...|\n",
      "|  1|AI helps humanity...|  1.0|[ai, helps, human...|[ai, helps, human...|(22,[6,8,10,15,20...|\n",
      "|  2|unsupervised lear...|  1.0|[unsupervised, le...|[unsupervised, le...|(22,[0,1,2,4,9,11...|\n",
      "|  3|i think unsupervi...|  0.0|[i, think, unsupe...|[think, unsupervi...|(22,[0,1,12,14],[...|\n",
      "|  4|machine learning ...|  0.0|[machine, learnin...|[machine, learnin...|(22,[0,3,5,16],[1...|\n",
      "|  5|machine learning ...|  1.0|[machine, learnin...|[machine, learnin...|(22,[0,2,3],[1.0,...|\n",
      "+---+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Print old dataset to show they are equal.\n",
    "dataset.show()\n",
    "# Clean the dataset if we rerun this part of the notebook.\n",
    "dataset = dataset.select([dataset.id, dataset.text, dataset.label])\n",
    "preprocessingPipeline = Pipeline(stages=[tokenizer, stopWordRemover, cv, cvModel])\n",
    "preprocessedModel = preprocessingPipeline.fit(dataset)\n",
    "dataset = preprocessedModel.transform(dataset)\n",
    "# Show the output of the preprocessing pipeline.\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  0|      skynet is here|  0.0|\n",
      "|  1|unsupervised lear...|  1.0|\n",
      "+---+--------------------+-----+\n",
      "\n",
      "DataFrame after applying the model and preprocessing pipeline:\n",
      "+----------+-----+--------------------+\n",
      "|prediction|label|         probability|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|[0.67889908256880...|\n",
      "|       1.0|  1.0|[0.37134813750430...|\n",
      "+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Magically fetch new comments (with a label) in the same format as the new dataset.\n",
    "testset = sqlContext.createDataFrame([\n",
    "    (0L, \"skynet is here\", 0.0),\n",
    "    (1L, \"unsupervised learning is very cool\", 1.0)], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Display the testset before any preprocessing steps.\n",
    "print(\"Original DataFrame:\")\n",
    "testset.show()\n",
    "# Feed the set set to the previously created preprocessing pipeline.\n",
    "preprocessedModel = preprocessingPipeline.fit(testset)\n",
    "testset = preprocessedModel.transform(testset)\n",
    "\n",
    "result = mlModel.transform(testset)\n",
    "\n",
    "# Show the label and prediction after applying the machine learning model.\n",
    "print(\"DataFrame after applying the model and preprocessing pipeline:\")\n",
    "result.select([result.prediction, result.label, result.probability]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
